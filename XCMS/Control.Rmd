---
title: "Falkor Control Script"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, error = TRUE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
library(tidyverse)
library(data.table)
library(pbapply)
library(xcms)
```

```{r specific_setup}
polarity <- "pos"

pretty_folder <- paste0("XCMS/", polarity, "_pretty/")
if(!dir.exists(pretty_folder)){dir.create(pretty_folder)}
intermediate_folder <- paste0("XCMS/", polarity, "_intermediate/")
if(!dir.exists(intermediate_folder)){dir.create(intermediate_folder)}

source("XCMS/scripts/functions.R")

ms_files <- "mzMLs/" %>%
  paste0(polarity) %>%
  list.files(pattern = ".mzML", full.names = TRUE) 

register(BPPARAM = SnowParam(tasks = length(ms_files), progressbar = TRUE))
```

## Setup and metadata things

The raw data is stored on the Synology drive in the Ingalls Lab as .raw file types. This data isn't super useful in its proprietary format, so it's easiest to convert those .raw files into .mzMLs with the "RunMsconvert.cmd" script in the project working directory. We write these files into the "mzMLs" folder in the working directory since they're also accessed by MSDIAL.

The code calls msconvert, a command-line utility from Proteowizard, to convert the files into mzML format. It also splits them into positive and negative mode and separates out the MSMS information into a sub-folder. Per Katherine's recommendation, I'm using blanks from her Gradients 2.0 project rather than the ones Laura added for me.

We also need the metadata that goes with these files, containing information about treatment type, which are blanks, injection volume, etc. It's also good to pull down the standards file from the lab Github during this setup phase. The small script below does this by accessing the file names, the instrument-produced sample key, and the internet.

```{r metadatagrab, eval=FALSE}
# Metadata
file_paths <- ms_files %>%
  normalizePath() %>%
  `[`(!grepl("Fullneg|Fullpos|QC-KM1906", x = .)) %>%
  `[`(!grepl("DDApos", x = .)) %>%
  `[`(!grepl("180205", x = .))
metadframe <- data.frame(
  file_name=basename(file_paths),
  depth=regmatches(regexpr(pattern = "Std|Poo|Blk|DCM|25m", file_paths), x = file_paths),
  station=regmatches(regexpr(pattern = "Std|Poo|Blk|S62|S64|S77|S80|L1|L2", file_paths), 
                     x = file_paths)
)
station_spindirs <- c(Blk="Blk", S62="Cyclone", S64="Cyclone", 
                      S77="Anticyclone", S80="Anticyclone",
                      L1="Cyclone", L2="Anticyclone",
                      Poo="Poo", Std="Std")
metadframe$spindir=station_spindirs[metadframe$station]
station_timing <- c(Blk="Blk", S62="4pm", S64="4am", 
                      S77="4pm", S80="4am",
                      L1="Various", L2="Various",
                      Poo="Poo", Std="Std")
metadframe$time=station_timing[metadframe$station]

cruise_type <- c(S62="Falkor", S77="Falkor", S64="Falkor", S80="Falkor",
                 L1="Mesoscope", L2="Mesoscope", Poo="Poo", Std="Std",
                 Blk="Blk")
metadframe$cruise=cruise_type[metadframe$station]
write.csv(x = metadframe, row.names = FALSE, file = "XCMS/falkor_metadata.csv")
head(metadframe)



# Sample key information
sample_key <- read.csv(paste0("Z:\\1_QEdata\\LTC\\DATA\\HILIC\\190718_Depth",
                              "Profiles_FK180310\\Sample.Key.HILIC.csv"),
                       skip = 1)
sample_key_meso <- read.csv(paste0("Z:\\1_QEdata\\LTC\\DATA\\HILIC\\HILIC_2018\\",
                                   "180205_Wei_MESO-SCOPE_HRM_DepthProfile\\",
                                   "negative\\Sample.Key.csv"))
falkor_key <- sample_key %>%
  filter(grepl(pattern = "FK|Std", sample_key$Sample.Name)) %>%
  select(Sample.Name, Inj.vol) %>%
  add_row(Sample.Name="170706_Blk_Blk0p2_1", Inj.vol=1) %>%
  add_row(Sample.Name="170706_Blk_Blk0p2_2", Inj.vol=1) %>%
  add_row(Sample.Name="190715_Blk_KM1906U14-Blk_C", Inj.vol=1) %>%
  mutate(fileid=paste0(Sample.Name, ".mzML")) %>%
  select(-Sample.Name, inj_vol=Inj.vol, file_name=fileid) %>%
  arrange(file_name)
meso_key <- sample_key_meso %>%
  filter(grepl(pattern = "Smp|Std|TruePoo", sample_key_meso$Sample.Name)) %>%
  filter(Fraction=="HILIC") %>%
  select(Sample.Name, Inj.vol=Bio.Normalization) %>%
  add_row(Sample.Name="180205_Std_4uMStdsInH2O_1.mzML", Inj.vol=1) %>%
  add_row(Sample.Name="180205_Std_4uMStdsInH2O_2.mzML", Inj.vol=1) %>%
  add_row(Sample.Name="180205_Std_4uMStdsInMatrix_1.mzML", Inj.vol=1) %>%
  add_row(Sample.Name="180205_Std_4uMStdsInMatrix_2.mzML", Inj.vol=1) %>%
  mutate(fileid=paste0(Sample.Name, ".mzML")) %>%
  select(-Sample.Name, inj_vol=Inj.vol, file_name=fileid) %>%
  arrange(file_name)
falkor_key <- rbind(falkor_key, meso_key)
write.csv(falkor_key, file = "XCMS/falkor_sample_key.csv", row.names = FALSE)
head(falkor_key)


# Standards information
raw_stans <- read.csv(paste0("https://raw.githubusercontent.com/",
                             "IngallsLabUW/Ingalls_Standards/master/",
                             "Ingalls_Lab_Standards_NEW.csv"))
falkor_stans <- raw_stans %>%
  filter(Column=="HILIC") %>%
  mutate(polarity=tolower(gsub(pattern = "HILIC", "", .$Fraction1))) %>%
  select(compound_type=Compound.Type, compound_name=Compound.Name, 
         formula=Emperical.Formula, rt=RT..min., mz=m.z, ionization_form,
         charge=z, kegg_id=C0, polarity, date_added=Date.added, mix=HILICMix)
meso_stans <- falkor_stans %>%
  filter(date_added<190000)
write.csv(falkor_stans, file = "XCMS/falkor_stans.csv", row.names = FALSE)
write.csv(meso_stans, file = "XCMS/meso_stans.csv", row.names = FALSE)
head(meso_stans)
```

## Peakpicking

**Obviously** the most important step, you can't do anything else without first finding things to look at and talk about. The script below, when sourced, runs XCMS's centWave peakpicking. It also calculates an improved signal-to-noise and metric of Gaussian-ness that seems to sift through the noise more accurately than the default signal-to-noise ratio, which has known bugs (https://doi.org/10.1021/acs.analchem.7b01069). Finally, it performs retention time correction and peak correspondence (grouping peaks across files).

It takes in the metadataframe created above as well as the list of paths to the .mzML files and returns a (rather large) data frame full of peaks. 

```{r Peakpicking, error=TRUE}

falkor_metadata <- read.csv("XCMS/falkor_metadata.csv")
ms_files <- falkor_metadata$file_name

# Define the peakpicking parameters
cwp <- CentWaveParam(ppm = 2.5, peakwidth = c(15, 15), 
                     snthresh = 1, prefilter = c(0, 10000), 
                     integrate = 2, mzCenterFun = "wMean", 
                     mzdiff = 0.001, fitgauss = FALSE, 
                     noise = 5000, firstBaselineCheck = FALSE, 
                     extendLengthMSW = TRUE)

# Set the new quality threshold
qscore_threshold <- 20

# Define the retention time correction parameters
obp <- ObiwarpParam(binSize = 0.1, centerSample = 5, 
                    response = 1, distFun = "cor_opt")

# Define the correspondence parameters
pdp <- PeakDensityParam(sampleGroups = falkor_metadata$spindir, 
                        bw = 5, minFraction = 0.1, 
                        binSize = 0.001, minSamples = 2)

# Use the default peak filling parameters (no mz/rt expansion)
fpp <- FillChromPeaksParam()



source("XCMS/scripts/peakpicking.R")

saveRDS(xdata, file = paste0(intermediate_folder, "dirty_xdata.rds"))
saveRDS(xdata_filled, file = paste0(intermediate_folder, "xdata_filled.rds"))
print(xdata_filled)
write.csv(raw_peaks, file = paste0(intermediate_folder, "raw_peaks.csv"), row.names = FALSE)
head(raw_peaks)
```

## De-isotoping and de-adducting

After peaks have been identified, many of them will be isotopes and adducts of other peaks. Removing these is important to minimize pseudoreplication issues and data artifacts. I didn't like any of the packages I tried to do this, so I wrote my own code to process this more robustly.

This script does two things. First, it identifies peaks that are likely isotopes or adducts of other peaks in the data set. It does this by assuming that every single peak is an isotope/adduct of another peak. If a peak is an adduct/isotope, there will be another peak that looks very similar but is separated by a very specific mass difference. For example, if we find a peak at 140.06875, we "guess" that it's an adduct or isotope and check all the places where the M+H would be found, depending on the isotope or adduct. If we suspect it of being a 13C isotope of another peak, we'd look for a peak at 140.06875-1.003355. If we suspect it of being a sodium isotope, we'd look for a peak at 140.06875-22.99787+1.007276. In this case, we would indeed find a signal at the M+H peak if we assume sodium, but no peak at the M+H if we assume a 13C isotope - allowing us to conclude tentatively that this is actually an adduct.

However, simply finding data at the expected mass isn't specific enough because we'll often stumble upon noise or a different peak in the general area. Thus, we check for peak similarity before assuming that there's real signal there. The general theory is that isotopes and adducts will be similar to the base peak in two ways. First, the individual x/y (rt/intensity) data points should match up almost exactly, as adducts and isotopes elute at the same time as the base peak. We can check this with a Pearson correlation - simply, how nicely do the two peaks correlate with each other? This method is quite sensitive to even small differences in retention time. Second, we can compare peak ratios across files. Here, we run a similar correlation but check x/y as base peak/isotope peak. This works because a compound will always create similarly stable adducts (some will love M+H, others M+Na) and will always have a fixed isotope abundance. Across multiple files, then, the correlation between these should be very strong (and is, in fact, usually stronger than the first method).

```{r deisoadduct}
xdata_filled <- readRDS(file = paste0(intermediate_folder, "xdata_filled.rds"))
raw_peaks <- read.csv(file = paste0(intermediate_folder, "raw_peaks.csv"))

not_addisos <- list("Glutamine"=c(mz=147.076968, rt=620),
                    "Citrulline"=c(mz=176.103517, rt=645),
                    "Guanine"=c(mz=152.0567, rt=400))


# When removing peaks that are likely adducts...
# How similar do the median peak and median adduct need to be to assume adduct?
shape_remove_threshold <- 0.8
# How good does the peak area ~ adduct area correlation across files need to be to assume adduct?
area_remove_threshold <- 0.99

# When finding adducts and isotopes of a given peak...
# How similar do the median peak and median adduct need to be to assume adduct?
# Typically lower than above because priors are better
shape_find_threshold <- 0.75
# How good does the peak area ~ adduct area correlation across files need to be to assume adduct?
area_find_threshold <- 0.9

source("XCMS/scripts/deisoadduct.R")

# Write out data frame containing all adducts and isotopes
write.csv(addiso_peaks, file = paste0(pretty_folder, "addiso_peaks.csv"), row.names = FALSE)
head(addiso_peaks)

# Write out data frame containing all "real" peaks
write.csv(real_peaks, file = paste0(intermediate_folder, "real_peaks.csv"), row.names = FALSE)
head(real_peaks)

# Write out summary data frame containing deconvoluted MS1 information
write.csv(real_features, file = paste0(pretty_folder, "complete_features.csv"), row.names = FALSE)
head(real_features)
```

## Finding B-MISs

```{r B-MIS}
addiso_peaks <- read.csv(paste0(pretty_folder, "addiso_peaks.csv")) %>%
  select(feature, mz, rt, into, file_name, M_area)
real_peaks <- read.csv(paste0(intermediate_folder, "real_peaks.csv")) %>%
  select(feature, mz, rt, into, file_name, M_area)
all_peaks <- rbind(addiso_peaks, real_peaks) %>%
  arrange("feature", "file_name")

stan_list <- read.csv("XCMS/meso_stans.csv")
stan_list <- read.csv("XCMS/falkor_stans.csv")
bionorm_values <- read.csv("XCMS/falkor_sample_key.csv")
cut.off <- 0.4 #Necessary improvement for "acceptable"
cut.off2 <- 0.1 #If RSD already below, skip B-MIS

source("XCMS/scripts/bmisscript.R")

write.csv(found_stans, file = paste0(intermediate_folder, "found_stans.csv"), 
          row.names = FALSE)
write.csv(BMISed_feature_peaks, file = paste0(pretty_folder, "BMISed_peaks.csv"), 
          row.names = FALSE)

BMISed_feature_peaks %>%
  group_by(feature) %>%
  summarize(BMIS=unique(BMIS)) %>%
  pull(BMIS) %>% table()

message(Sys.time()-start_time)
```

## Combining positive and negative modes

```{r comb, eval=FALSE}

```

## Assign formulae and structures

```{r assign}
falkor_metadata <- read.csv("XCMS/falkor_metadata.csv")
final_features <- read.csv(paste0(pretty_folder, "complete_features.csv"))
final_peaks <- read.csv(paste0(intermediate_folder, "real_peaks.csv"))

sirius_project_dir <- paste0(intermediate_folder, "/sirius_project")

MSMS_files <- paste0("mzMLs/", polarity, "/MSMS/") %>%
  list.files(pattern = ".mzML", full.names = TRUE) %>%
  normalizePath() %>%
  `[`(grepl("pos", x = .)) %>%
  `[`(!grepl("180205", x = .))
register(BPPARAM = SnowParam(tasks = length(MSMS_files), progressbar = TRUE))

source("XCMS/scripts/formula_assignments.R")

all_stans <- read.csv("XCMS/falkor_stans.csv") %>% 
  filter(polarity=="pos") %>% filter(!is.na(mix))
# source("XCMS/scripts/standard_assignments.R")

write.csv(x = best_formulas, file = paste0(pretty_folder, "feature_formulas.csv"))
```

## Data analysis!

```{r analysis}
falkor_metadata <- read.csv(file = "XCMS/falkor_metadata.csv")
found_stans <- read.csv(file = paste0(intermediate_folder, "found_stans.csv"))
final_peaks <- read.csv(file = paste0(pretty_folder, "BMISed_peaks.csv")) %>%
  left_join(falkor_metadata)

final_peaks <- final_peaks %>% 
  filter(!feature%in%found_stans$feature)

diffreport <- function(peaks, pattern_a, pattern_b){
  split(peaks, peaks$feature) %>%
    lapply(function(x){
      DCM_areas <- x$BMISed_area[grep(pattern = pattern_a, x$file_name)]
      m25_areas <- x$BMISed_area[grep(pattern = pattern_b, x$file_name)]
      if(all(DCM_areas==0)&all(m25_areas==0))return(c(1, 1))
      if(length(DCM_areas)<3|length(m25_areas)<3)return(c(1, 1))
      c(pval=t.test(DCM_areas, m25_areas)$p.value, 
        diff=mean(DCM_areas, na.rm=TRUE)/mean(m25_areas, na.rm=TRUE))
    }) %>% do.call(what = rbind) %>% as.data.frame() %>%
    mutate(feature=rownames(.)) %>%
    mutate(p_adj=p.adjust(pval, method = "BH")) %>%
    filter(p_adj<0.05) %>%
    arrange(p_adj) %>%
    mutate(which_enriched=c(pattern_a, pattern_b)[as.numeric(diff>1)+1])
}

# Depth data!
depth_diffs <- diffreport(final_peaks, pattern_a = "DCM", pattern_b = "25m") %>%
  left_join(final_peaks %>% group_by(feature) %>%
              summarize(mzmed=median(mz), rtmed=median(rt)), by="feature")

# Diel data!
diffreport(final_peaks, pattern_a = "62|77", pattern_b = "64|80")

# Direction data!
diffreport(final_peaks, pattern_a = "62|64|L1", pattern_b = "77|80|L2") %>%
  left_join(final_peaks %>% group_by(feature) %>%
              summarize(mzmed=median(mz), rtmed=median(rt)), by="feature")

depth_diffs$feature[1] %>%
  lapply(function(feature_num){
    gp_df <- final_peaks %>%
      filter(feature==feature_num) %>%
      #filter(depth%in%c("25m", "DCM")) %>%
      #filter(spindir%in%c("Cyclone", "Anticyclone")) %>%
      select(`Sample type`=depth, everything())
    gp <- gp_df %>%
      ggplot() + 
      geom_boxplot(
        aes(x=`Sample type`, y=BMISed_area, color=`Sample type`), 
        varwidth = TRUE) +
      theme_bw() + theme(legend.position = "none") + 
      ylab("BMIS-normalized peak area") +
      annotate(geom = "label", x=Inf, y=Inf, label=unique(gp_df$feature), hjust=1, vjust=1) +
      annotate(geom = "label", x=-Inf, y=Inf, label=median(gp_df$mz), hjust=0, vjust=1)
    print(gp)
  })
```
